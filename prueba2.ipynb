{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e5040b7-b5ba-4667-b9e2-d3ebf7f86fb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: feedparser in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (6.0.11)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from feedparser) (1.0.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from sentence-transformers) (0.31.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\anaconda3\\envs\\entrono_prueba\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "    # ======================\n",
    "    # 1. Instalar librerías\n",
    "    # ======================\n",
    "    !pip install feedparser beautifulsoup4 requests pandas sentence-transformers scikit-learn openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b4f3973-f308-41e1-8668-54a9dab2f6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recolección:  39%|██████████████████████████▋                                          | 22/57 [00:41<02:17,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en scraping https://www.nocheyniebla.org/: HTTPSConnectionPool(host='www.nocheyniebla.org', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x00000259F5E69A60>, 'Connection to www.nocheyniebla.org timed out. (connect timeout=10)'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recolección: 100%|█████████████████████████████████████████████████████████████████████| 57/57 [01:19<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noticias antes de eliminar duplicados: 196\n",
      "Noticias después de eliminar duplicados: 195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asignando ubicación:   0%|                                                                     | 0/195 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'normalizar_texto' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 244\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_final\n\u001b[0;32m    241\u001b[0m \u001b[38;5;66;03m# =======================================\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# 11 Ejecutar\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# =======================================\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m noticias_final \u001b[38;5;241m=\u001b[39m ejecutar_pipeline(medios_df, palabras_clave, df_mpios)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;66;03m#noticias_final.to_excel(\"noticias_resultado.xlsx\", index=False)\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m#noticias_final.to_excel(f\"noticias_resultado_{datetime.now().strftime('%Y-%m-%d')}.xlsx\", index=False)\u001b[39;00m\n\u001b[0;32m    247\u001b[0m noticias_final\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresultados/noticias_resultado_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[1], line 225\u001b[0m, in \u001b[0;36mejecutar_pipeline\u001b[1;34m(medios_df, palabras_clave, df_mpios)\u001b[0m\n\u001b[0;32m    222\u001b[0m noticias_filtradas \u001b[38;5;241m=\u001b[39m eliminar_duplicados(noticias_df)\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNoticias después de eliminar duplicados: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(noticias_filtradas)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 225\u001b[0m noticias_enriquecidas \u001b[38;5;241m=\u001b[39m [asignar_ubicacion(n, df_mpios) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m tqdm(noticias_filtradas, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsignando ubicación\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m    227\u001b[0m columnas_finales \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedio\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitulo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfecha\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlink\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmunicipio\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcod_municipio\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    228\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepartamento\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcod_departamento\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpalabra_clave\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontenido\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    230\u001b[0m df_final \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(noticias_enriquecidas)[columnas_finales]\n",
      "Cell \u001b[1;32mIn[1], line 121\u001b[0m, in \u001b[0;36masignar_ubicacion\u001b[1;34m(noticia, df_mpios)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21masignar_ubicacion\u001b[39m(noticia, df_mpios):\n\u001b[0;32m    120\u001b[0m     contexto_territorio \u001b[38;5;241m=\u001b[39m df_contexto[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontexto\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m--> 121\u001b[0m     texto \u001b[38;5;241m=\u001b[39m normalizar_texto(noticia[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitulo\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m noticia[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontenido\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;66;03m# Revisar combinaciones más confiables: municipio seguido de departamento\u001b[39;00m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df_mpios\u001b[38;5;241m.\u001b[39miterrows():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'normalizar_texto' is not defined"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# 2. Importar librerías\n",
    "# ======================\n",
    "import feedparser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import html\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 3. Cargar archivos base\n",
    "# ======================\n",
    "medios_df = pd.read_excel(\"medios_colombia.xlsx\")#Contiene la información de medios de comunicación\n",
    "df_palabras = pd.read_excel(\"palabras_clave.xlsx\")#Contiene las palabras clave para filtrar las noticias\n",
    "palabras_clave = df_palabras['palabra_clave'].dropna().str.lower().tolist()\n",
    "df_mpios = pd.read_excel(\"municipio.xlsx\")#Contiene los departamentos y municipios según el DANE\n",
    "df_contexto = pd.read_excel(\"contexto_territorio.xlsx\")#Contiene palabras de contexto territorial para afinar la asignación de territorio a la noticia\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 4. Funciones de recolección de noticias (rss o scraping)\n",
    "# ============================\n",
    "def extraer_rss(url):\n",
    "    try:\n",
    "        feed = feedparser.parse(url)\n",
    "        noticias = []\n",
    "        for entry in feed.entries:\n",
    "            titulo = entry.title\n",
    "            link = entry.link\n",
    "            fecha = entry.get('published', '') or entry.get('updated', '')\n",
    "            contenido_raw = entry.get('summary', '') or entry.get('description', '')\n",
    "            contenido = limpiar_contenido_html(contenido_raw)\n",
    "            noticias.append({\n",
    "                'titulo': titulo,\n",
    "                'fecha': datetime.now().isoformat(),\n",
    "                'link': link,\n",
    "                'contenido': contenido\n",
    "            })\n",
    "        return noticias\n",
    "    except Exception as e:\n",
    "        print(f\"Error en RSS {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def extraer_scraping(url, selector):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        elementos = soup.select(selector)\n",
    "        noticias = []\n",
    "        for e in elementos:\n",
    "            texto_bruto = str(e)\n",
    "            texto_limpio = limpiar_contenido_html(texto_bruto)\n",
    "            if texto_limpio:\n",
    "                noticias.append({\n",
    "                    'titulo': texto_limpio[:100],  # puedes ajustar cómo se define el título\n",
    "                    'fecha': datetime.now().isoformat(),\n",
    "                    'link': url,\n",
    "                    'contenido': texto_limpio\n",
    "                })\n",
    "        return noticias\n",
    "    except Exception as e:\n",
    "        print(f\"Error en scraping {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 5. Filtrar noticias por palabras clave\n",
    "# =======================================\n",
    "def extraer_palabras_clave(texto, palabras_clave):\n",
    "    texto = texto.lower()\n",
    "    coincidencias = [palabra for palabra in palabras_clave if palabra in texto]\n",
    "    return coincidencias if coincidencias else None\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 6. Eliminar duplicados por similitud\n",
    "# =======================================\n",
    "def eliminar_duplicados(lista_noticias, umbral=0.8):\n",
    "\n",
    "    # Convertir lista de dicts a DataFrame\n",
    "    df = pd.DataFrame(lista_noticias)\n",
    "\n",
    "    # Concatenar título + contenido para tener mayor contexto\n",
    "    textos = (df['titulo'].fillna('') + ' ' + df['contenido'].fillna('')).tolist()\n",
    "\n",
    "    # Cargar modelo semántico\n",
    "    modelo = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = modelo.encode(textos, convert_to_tensor=True)\n",
    "\n",
    "    # Calcular similitud coseno\n",
    "    similitudes = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "    # Marcar duplicados (solo superior a la diagonal)\n",
    "    duplicados = set()\n",
    "    for i in range(len(similitudes)):\n",
    "        for j in range(i + 1, len(similitudes)):\n",
    "            if similitudes[i][j] > umbral:\n",
    "                duplicados.add(j)\n",
    "\n",
    "    # Filtrar no duplicados\n",
    "    df_filtrado = df.drop(index=list(duplicados)).reset_index(drop=True)\n",
    "\n",
    "    # Convertir de nuevo a lista de dicts\n",
    "    return df_filtrado.to_dict(orient='records')\n",
    "\n",
    "# Normalizar texto\n",
    "def normalizar_texto(texto):\n",
    "    if not isinstance(texto, str):\n",
    "        return ''\n",
    "    texto = texto.lower()\n",
    "    texto = unicodedata.normalize('NFKD', texto).encode('ASCII', 'ignore').decode('utf-8')\n",
    "    texto = re.sub(r'[^\\w\\s]', '', texto)  # elimina signos de puntuación\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()  # elimina espacios múltiples\n",
    "    return texto\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 7. Asignar municipio y departamento (mejorado)\n",
    "# =======================================\n",
    "def asignar_ubicacion(noticia, df_mpios):\n",
    "    contexto_territorio = df_contexto['contexto'].dropna().str.strip().str.lower().tolist()\n",
    "    texto = normalizar_texto(noticia['titulo'] + \" \" + noticia['contenido'])\n",
    "\n",
    "    # Revisar combinaciones más confiables: municipio seguido de departamento\n",
    "    for _, row in df_mpios.iterrows():\n",
    "        municipio = normalizar_texto(str(row['municipio']))\n",
    "        departamento = normalizar_texto(str(row['departamento']))\n",
    "\n",
    "        # Ejemplo: \"mallama, nariño\"\n",
    "        patron_comb = r'\\b' + re.escape(municipio) + r'\\s*,\\s*' + re.escape(departamento) + r'\\b'\n",
    "        if re.search(patron_comb, texto):\n",
    "            noticia['municipio'] = row['municipio']\n",
    "            noticia['departamento'] = row['departamento']\n",
    "            noticia['cod_municipio'] = row['cod_municipio']\n",
    "            noticia['cod_departamento'] = row['cod_departamento']\n",
    "            return noticia\n",
    "\n",
    "    # Buscar municipios con contexto\n",
    "    for _, row in df_mpios.iterrows():\n",
    "        municipio = normalizar_texto(str(row['municipio']))\n",
    "        for contexto in contexto_territorio:\n",
    "            patron = r'\\b' + re.escape(contexto) + r'\\s+' + re.escape(municipio) + r'\\b'\n",
    "            if re.search(patron, texto):\n",
    "                noticia['municipio'] = row['municipio']\n",
    "                noticia['departamento'] = row['departamento']\n",
    "                noticia['cod_municipio'] = row['cod_municipio']\n",
    "                noticia['cod_departamento'] = row['cod_departamento']\n",
    "                return noticia\n",
    "\n",
    "    # Buscar departamentos con contexto (solo si no se encontró municipio)\n",
    "    for _, row in df_mpios.iterrows():\n",
    "        departamento = normalizar_texto(str(row['departamento']))\n",
    "        for contexto in contexto_territorio:\n",
    "            patron = r'\\b' + re.escape(contexto) + r'\\s+' + re.escape(departamento) + r'\\b'\n",
    "            if re.search(patron, texto):\n",
    "                noticia['municipio'] = None\n",
    "                noticia['departamento'] = row['departamento']\n",
    "                noticia['cod_municipio'] = None\n",
    "                noticia['cod_departamento'] = row['cod_departamento']\n",
    "                return noticia\n",
    "\n",
    "    # Si no se encuentra nada\n",
    "    noticia['municipio'] = None\n",
    "    noticia['departamento'] = None\n",
    "    noticia['cod_municipio'] = None\n",
    "    noticia['cod_departamento'] = None\n",
    "    return noticia\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 8. Estandariza la fecha\n",
    "# =======================================\n",
    "def estandarizar_fecha(fecha):\n",
    "    \"\"\"\n",
    "    Convierte una fecha en múltiples formatos posibles al formato DD/MM/AAAA.\n",
    "    Si no se puede interpretar, devuelve None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fecha_dt = parser.parse(str(fecha), dayfirst=True, fuzzy=True)\n",
    "        return fecha_dt.strftime('%d/%m/%Y')\n",
    "    except (parser.ParserError, TypeError, ValueError):\n",
    "        return None\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 9. limpiar campo \"contenido\"\n",
    "# =======================================\n",
    "def limpiar_contenido_html(texto):\n",
    "    texto_limpio = BeautifulSoup(texto, \"html.parser\").get_text(separator=' ', strip=True)\n",
    "    texto_limpio = html.unescape(texto_limpio)\n",
    "    texto_limpio = re.sub(r'\\s+', ' ', texto_limpio).strip()\n",
    "    return texto_limpio\n",
    "\n",
    "    \n",
    "# =======================================\n",
    "# 10. Función principal\n",
    "# =======================================\n",
    "def ejecutar_pipeline(medios_df, palabras_clave, df_mpios):\n",
    "    noticias_totales = []\n",
    "\n",
    "    for _, row in tqdm(medios_df.iterrows(), total=len(medios_df), desc=\"Recolección\"):\n",
    "        tipo = row['tipo']\n",
    "        url = row['url']\n",
    "        medio = row['nombre_medio']\n",
    "        selector = row.get('selector_scraping', None)\n",
    "\n",
    "        if tipo.lower() == 'rss':\n",
    "            noticias = extraer_rss(url)\n",
    "        elif tipo.lower() == 'scrap' and pd.notna(selector):\n",
    "            noticias = extraer_scraping(url, selector)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        for noticia in noticias:\n",
    "            noticia['medio'] = medio\n",
    "            palabras_encontradas = extraer_palabras_clave(noticia['titulo'] + \" \" + noticia['contenido'], palabras_clave)\n",
    "            if palabras_encontradas:\n",
    "                noticia['palabra_clave'] = ', '.join(palabras_encontradas)  \n",
    "                noticias_totales.append(noticia)\n",
    "\n",
    "    print(f\"Noticias antes de eliminar duplicados: {len(noticias_totales)}\")\n",
    "    noticias_df = pd.DataFrame(noticias_totales)\n",
    "    noticias_filtradas = eliminar_duplicados(noticias_df)\n",
    "    print(f\"Noticias después de eliminar duplicados: {len(noticias_filtradas)}\")\n",
    "\n",
    "    noticias_enriquecidas = [asignar_ubicacion(n, df_mpios) for n in tqdm(noticias_filtradas, desc=\"Asignando ubicación\")]\n",
    "    \n",
    "    columnas_finales = [\"medio\", \"titulo\", \"fecha\", \"link\", \"municipio\", \"cod_municipio\",\n",
    "                        \"departamento\", \"cod_departamento\", \"palabra_clave\", \"contenido\"]\n",
    "    \n",
    "    df_final = pd.DataFrame(noticias_enriquecidas)[columnas_finales]\n",
    "    \n",
    "    # Seleccionar los datos del día\n",
    "    df_final['fecha'] = df_final['fecha'].apply(estandarizar_fecha)\n",
    "    df_final['fecha'] = pd.to_datetime(df_final['fecha'], format='%d/%m/%Y', errors='coerce')\n",
    "    hoy = pd.to_datetime(datetime.now().strftime('%d/%m/%Y'), format='%d/%m/%Y')\n",
    "    df_hoy = df_final[df_final['fecha'] == hoy]\n",
    "\n",
    "    return df_final\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 11 Ejecutar\n",
    "# =======================================\n",
    "noticias_final = ejecutar_pipeline(medios_df, palabras_clave, df_mpios)\n",
    "#noticias_final.to_excel(\"noticias_resultado.xlsx\", index=False)\n",
    "#noticias_final.to_excel(f\"noticias_resultado_{datetime.now().strftime('%Y-%m-%d')}.xlsx\", index=False)\n",
    "noticias_final.to_excel(f\"resultados/noticias_resultado_{datetime.now().strftime('%Y-%m-%d')}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2910e27-842f-464e-8a80-549979e9b661",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
